#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k],1e-8))
}
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k],1e-8))
}
print(log_likelihoods)
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
adjusted_rand_score()
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k],1e-8)))
}
print(log_likelihoods)
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
}
}
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
# Check dimensions of X, F, and pmax(1 - F[, k], thr)
cat("Dimensions of X:", dim(X), "\n")
cat("Dimensions of F:", dim(F), "\n")
cat("Dimensions of pmax(1 - F[, k], thr):", dim(pmax(1 - F[, k], thr)), "\n")
# Update log_likelihoods
log_likelihoods[, k] <- (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
# Confirm the dimensions of log_likelihoods
cat("Dimensions of log_likelihoods:", dim(log_likelihoods), "\n")
for (k in 1:K) {
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log(pmax(1 - F[, k], thr))
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
adjusted_rand_score()
#X: N by M data matrix
#params: a list with two parameters returned from M_step
E_step = function (X, params, thr = 10**(-8)){
F = params$"F" #Frequency matrix F: M by K
pis = params$"pis" #Proportion vector pi: length K
N = dim(X)[1]; M = dim(X)[2]
K = dim(F)[2]
######### TODO 3b: modify the following to have meaning updates #########
#calculate weighted_log_prob: log(P(X_i=x_i | Z_i=k, theta) * P(Z_i=k | theta))
#calculate log_prob_sample: log P(Xi=x_i | theta) N by 1. Hint: use logSumExp function
#calculate log_prob_data: logP(X_1:n=x_1:n | theta) scalar
#calculate log_gammas: log P(Z_i=k | X_i=x_i, theta) N by K
log_likelihoods = matrix(0, nrow = N, ncol = K)
for (k in 1:K) {
log_likelihoods[, k] = (X * log(F[, k]) + (1 - X) * log(pmax(1 - F[, k], thr)))
}
weighted_log_prob = matrix(0, nrow = N, ncol = K)
for (i in 1:N) {
for (k in 1:K) {
weighted_log_prob[i, k] = log_likelihoods[i, k] + log(pis[k])
}
}
log_prob_sample = logSumExp(weighted_log_prob)
log_prob_data = sum(logSumExp(log_likelihoods + log(pis)))
log_gammas = log_likelihoods + log(pis) - matrix(logSumExp(log_likelihoods + log(pis), axis = 2), N, K, byrow = TRUE)
######### end of modification #########
return(list("log_gammas" = log_gammas, "log_prob_data" = log_prob_data))
}
EM = function(X, K = 2, max_iter = 100, tol = 10**(-4), n_init = 3, debug = FALSE){
N = dim(X)[1];  M = dim(X)[2]
res = list()
best_log_prob_data = -Inf
converged = FALSE
#loop through different random starting points
for (init in 1:n_init){
set.seed(init)
if(debug){print(paste0("starting EM on random initialization: ", init, " out of ", n_init))}
######### TODO 3c: modify the following to have the full EM updates #########
#initialize soft assignment
gammas = NULL
for (n_iter in 1:max_iter){
prev_log_prob_data = log_prob_data
######### convergence check #########
change = (log_prob_data - prev_log_prob_data)/N
if (abs(change) < tol){
if(debug){
print(paste0("random initialization ", init ," converged at iteration ", n_iter))
print("")
}
converged = TRUE
break
}
######### update on the best initialization #########
best_init = NULL
}
}
######### end of modification #########
res[["converged"]] = converged
res[["best_init"]] = best_init
return(res)
}
#Question 1
GMM<-M_step(X_df, Z_df)
#Question 2
E<-E_step(X_df,GMM)
adjusted_rand_score()
adjusted_rand_score()
i+1
1+1
